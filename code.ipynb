{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fd212-7ed3-41a7-9bd2-3a5e8858da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long version - does the entire processing from scratch.\n",
    "\n",
    "# ---------------------------- SETUP ---------------------------- #\n",
    "\n",
    "# Imports the necessary libraries\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "# Load OpenAI API key from environment or set manually\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") or \"your api key\"\n",
    "client = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Ensure API key is valid before proceeding\n",
    "if not openai_api_key or openai_api_key.startswith(\"your-\"):\n",
    "    raise ValueError(\"ERROR: OpenAI API key is missing or incorrect.\")\n",
    "\n",
    "print(f\"Using OpenAI API Key: {openai_api_key[:5]}********\")\n",
    "\n",
    "# File paths\n",
    "# Make sure you have the correct paths.\n",
    "metadata_file = \"/Code Execution/gutenberg_metadata.csv\"\n",
    "cleaned_file = \"/Code Execution/gutenberg_data_cleaned.csv\"\n",
    "sample_file = \"/Code Execution/gutenberg_sample.csv\"\n",
    "evaluation_file = \"/Code Execution/evaluation_results.csv\"\n",
    "\n",
    "# Load metadata\n",
    "df_metadata = pd.read_csv(metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe49e2-df05-47c0-ae86-7830468831af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- DATA CLEANING ---------------------------- #\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans raw book text by removing unwanted whitespace, footnotes, \n",
    "    and unnecessary headers such as the Project Gutenberg disclaimer.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw text from a book.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Removes newlines, tabs, and excess whitespace\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \")\n",
    "    text = \" \".join(text.split())  # Remove extra spaces\n",
    "    \n",
    "    # Remove Project Gutenberg headers and footers\n",
    "    start_marker = \"*** START OF THIS PROJECT GUTENBERG\"\n",
    "    end_marker = \"*** END OF THIS PROJECT GUTENBERG\"\n",
    "    \n",
    "    if start_marker in text and end_marker in text:\n",
    "        text = text.split(start_marker)[1].split(end_marker)[0]\n",
    "    return text\n",
    "\n",
    "def download_books():\n",
    "     \"\"\"\n",
    "    Downloads book texts from Project Gutenberg using provided metadata links.\n",
    "    Saves cleaned texts to a CSV file for further processing.\n",
    "\n",
    "    This function:\n",
    "    - Retrieves book text either from GutenbergPy or web scraping.\n",
    "    - Cleans the text using `clean_text()`.\n",
    "    - Saves the cleaned books in a CSV file.\n",
    "    \"\"\"\n",
    "    data = {\"Author\": [], \"Title\": [], \"Link\": [], \"ID\": [], \"Bookshelf\": [], \"Text\": []}\n",
    "\n",
    "    for index, row in df_metadata.iterrows():\n",
    "        try:\n",
    "            # Extracts book ID from URL\n",
    "            book_id = int(row[\"Link\"].split(\"/\")[-1])\n",
    "\n",
    "            # Retrieves text from Gutenberg website\n",
    "            page = requests.get(row[\"Link\"])\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            text_link = \"http://www.gutenberg.org\" + soup.find_all(\"a\", string=\"Plain Text UTF-8\")[0][\"href\"]\n",
    "            http_response_object = urlopen(text_link)\n",
    "            text = http_response_object.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # Clean the retrieved text\n",
    "            text = clean_text(text)\n",
    "\n",
    "            # Store book metadata and text\n",
    "            data[\"Text\"].append(text)\n",
    "            data[\"ID\"].append(book_id)\n",
    "            data[\"Title\"].append(row[\"Title\"])\n",
    "            data[\"Author\"].append(row[\"Author\"])\n",
    "            data[\"Link\"].append(row[\"Link\"])\n",
    "            data[\"Bookshelf\"].append(row[\"Bookshelf\"])\n",
    "        except:\n",
    "            print(f\"Could not download {row['Title']} (ID: {book_id})\")\n",
    "            continue\n",
    "\n",
    "    # Saves cleaned books to a csv file\n",
    "    df_cleaned = pd.DataFrame(data)\n",
    "    df_cleaned.to_csv(cleaned_file, index=False)\n",
    "    print(f\"Preprocessed dataset saved as {cleaned_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1d52d-553b-48d9-9ac6-afeafc09d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- CHUNKING STRATEGY ---------------------------- #\n",
    "\n",
    "def chunk_text(text, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks to maintain contextual continuity.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        chunk_size (int): Number of words per chunk.\n",
    "        overlap (int): Number of words overlapping between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfddfb7-bca8-45a2-a5cc-bcad18c07e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- VECTOR DATABASE (CHROMADB) ---------------------------- #\n",
    "\n",
    "# Loads sentence embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initializes ChromaDB client and create a collection for book embeddings\n",
    "chroma_client = chromadb.PersistentClient(path=\"chromadb_sample\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"gutenberg_sample\")\n",
    "\n",
    "\n",
    "def store_embeddings():\n",
    "    \"\"\"\n",
    "    Processes cleaned book texts, generates embeddings, and stores them in ChromaDB.\n",
    "\n",
    "    - Retrieves book text from `gutenberg_data_cleaned.csv`\n",
    "    - Splits text into chunks using `chunk_text()`\n",
    "    - Generates embeddings for each chunk\n",
    "    - Stores embeddings in ChromaDB\n",
    "    \"\"\"\n",
    "    df_cleaned = pd.read_csv(cleaned_file)\n",
    "\n",
    "    for index, row in df_cleaned.iterrows():\n",
    "        book_id = str(row[\"ID\"])\n",
    "        text = row[\"Text\"]\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "        embeddings = embedding_model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            collection.add(\n",
    "                ids=[f\"{book_id}_{i}\"],\n",
    "                embeddings=[embedding.tolist()],\n",
    "                metadatas=[{\"book_id\": book_id, \"chunk_index\": i, \"title\": row[\"Title\"], \"author\": row[\"Author\"]}],\n",
    "                documents=[chunk],\n",
    "            )\n",
    "    print(\"Embedding storage completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931e6b0-d505-4a57-b1c7-3b5f9a0f42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- QUERY EXPANSION & DOCUMENT RETRIEVAL ---------------------------- #\n",
    "def expand_query(query):\n",
    "     \"\"\"\n",
    "    Expands a query using GPT-4 to improve document retrieval.\n",
    "\n",
    "    Args:\n",
    "        query (str): Original user query.\n",
    "\n",
    "    Returns:\n",
    "        str: Expanded query with synonyms and related terms.\n",
    "    \"\"\"\n",
    "    prompt = f\"Rewrite the following query with synonyms and related terms to improve document retrieval:\\n\\nQuery: {query}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert in search query optimization.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def retrieve_documents(query, top_k=5):\n",
    "    \"\"\"Retrieves relevant documents from the vector database.\"\"\"\n",
    "    expanded_query = expand_query(query)\n",
    "    query_embedding = embedding_model.encode([expanded_query], convert_to_numpy=True)\n",
    "    results = collection.query(query_embeddings=query_embedding.tolist(), n_results=top_k)\n",
    "\n",
    "    return results[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c135f-5ae3-40ca-afb2-2e269475ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- INTERACTIVE QUERY SYSTEM ---------------------------- #\n",
    "\n",
    "def generate_response(query):\n",
    "    \"\"\"\n",
    "    Generates a response to the user's query based on retrieved book chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): User's input query.\n",
    "\n",
    "    Returns:\n",
    "        str: AI-generated response.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    prompt = f\"Answer the following query based only on the retrieved documents:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a research assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1fb08-372e-42d0-b8a0-13a349575434",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **6. Evaluation with Benchmarking**\n",
    "def evaluate_with_llm(query, expected, generated):\n",
    "    \"\"\"Compares the generated answer with the expected answer.\"\"\"\n",
    "    prompt = f\"Evaluate this response:\\n\\nQuery: {query}\\nExpected Answer: {expected}\\nGenerated Answer: {generated}\\n\\nRate it from 1 (poor) to 5 (excellent).\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "### **7. Interactive Querying**\n",
    "def interactive_query():\n",
    "    \"\"\"\n",
    "    Enables interactive user input for querying the system.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        query = input(\"Enter a query (or type 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        response = generate_response(query)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "\n",
    "# ---------------------------- RUN SYSTEM ---------------------------- #\n",
    "download_books()\n",
    "store_embeddings()\n",
    "interactive_query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 (TensorFlow)",
   "language": "python",
   "name": "pyenv_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
